def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold==True:
        text=text.casefold()
    if yo2e==True:
        text=text.replace('Ñ‘', 'e')
    text=text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    while '  ' in text:
        text=text.replace('  ', ' ')
    text=text.strip()
    return text
print(normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t"))
print(normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°"))
print(normalize("Hello\r\nWorld"))
print(normalize("  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  "))


def tokenize(text: str) -> list[str]:
    text_new=[]
    for i in range(len(text)):
        if text[i] == '-':
            if i > 0 and i < len(text) - 1:
                if (text[i-1].isalnum() or text[i-1] == '_') and (text[i+1].isalnum() or text[i+1] == '_'): 
                    text_new.append('_')
        else:
            text_new.append(text[i])
    text_new = ''.join(text_new)+' '
    word=''
    result=[]
    for x in text_new:
        if x.isalnum() or x=='_':
            word+=x
        else:
            if len(word)!=0:
                word=word.replace('_', '-')
                result.append(word)
                word=''
    return result
print(tokenize("Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€"))
print(tokenize("hello,world!!!"))
print(tokenize("Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾"))
print(tokenize("2025 Ð³Ð¾Ð´"))
print(tokenize("emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾"))

def count_freq(tokens: list[str]) -> dict[str, int]:
    result={}
    for i in tokens:
        result[i]=result.get(i, 0)+1
    sorted_dict={}
    s=sorted(result.keys())
    for key in s:
        sorted_dict[key]=result[key]
    return sorted_dict
print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "Ð¼Ð¸Ñ€", "Ð¿Ñ€Ð¸Ð²ÐµÑ‚"]))

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    spisoc=list(freq.items()) 
    sorted_spisoc=sorted(spisoc, key=lambda x: x[1], reverse=True)
    return sorted_spisoc[:n]
print(top_n({"bb": 2, "aa": 2, "cc": 1}))
print(top_n({"bb": 2, "aa": 2, "cc": 1}, n=2))